{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e483ef8c",
   "metadata": {},
   "source": [
    "## 根据年收入，是否有房，婚姻状况判断    是否拖欠欠款 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b33b1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "import operator\n",
    "import numpy as np\n",
    "\n",
    "def createDataSet():\n",
    "    \"\"\"构建数据集\"\"\"\n",
    "    dataSet = [['是', '单身', 125, '否'],\n",
    "               ['否', '已婚', 100, '否'],\n",
    "               ['否', '单身', 70, '否'],\n",
    "               ['是', '已婚', 120, '否'],\n",
    "               ['否', '离异', 95, '是'],\n",
    "               ['否', '已婚', 60, '否'],\n",
    "               ['是', '离异', 220, '否'],\n",
    "               ['否', '单身', 85, '是'],\n",
    "               ['否', '已婚', 75, '否'],\n",
    "               ['否', '单身', 90, '是']]\n",
    "    labels = ['是否有房', '婚姻状况', '年收入(k)']  # 三个特征\n",
    "    return dataSet, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b849d85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['单身', 90, '是']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "for featVec in  dataSet:       \n",
    "    currentLabel = featVec[-1]  #读取需要更新的类的具体明细\n",
    "    if currentLabel not in labelCounts.keys():   \n",
    "        labelCounts[currentLabel] =0\n",
    "    labelCounts[currentLabel] += 1\n",
    "shannonEnt = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a20be40d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "否\n",
      "否\n",
      "否\n",
      "否\n",
      "是\n",
      "否\n",
      "否\n",
      "是\n",
      "否\n",
      "是\n"
     ]
    }
   ],
   "source": [
    "dataSet = [['是', '单身', 125, '否'],\n",
    "           ['否', '已婚', 100, '否'],\n",
    "           ['否', '单身', 70, '否'],\n",
    "           ['是', '已婚', 120, '否'],\n",
    "           ['否', '离异', 95, '是'],\n",
    "           ['否', '已婚', 60, '否'],\n",
    "           ['是', '离异', 220, '否'],\n",
    "           ['否', '单身', 85, '是'],\n",
    "           ['否', '已婚', 75, '否'],\n",
    "           ['否', '单身', 90, '是']]\n",
    "for featVec in  dataSet:  \n",
    "    print(featVec[-1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87570555",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['是', '单身', 125, '否'],\n",
       "  ['否', '已婚', 100, '否'],\n",
       "  ['否', '单身', 70, '否'],\n",
       "  ['是', '已婚', 120, '否'],\n",
       "  ['否', '离异', 95, '是'],\n",
       "  ['否', '已婚', 60, '否'],\n",
       "  ['是', '离异', 220, '否'],\n",
       "  ['否', '单身', 85, '是'],\n",
       "  ['否', '已婚', 75, '否'],\n",
       "  ['否', '单身', 90, '是']],\n",
       " ['是否有房', '婚姻状况', '年收入(k)'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "createDataSet() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c009ccce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values(['是真的', '也是真的', '在一起'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aiqing={\"aini\":\"是真的\",\"henni\":\"也是真的\"}\n",
    "aiqing[\"aini\"]\n",
    "aiqing[\"zuizhong\"]=\"在一起\"\n",
    "aiqing.keys()\n",
    "aiqing.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2eb136e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aini:是真的\n",
      "henni:也是真的\n",
      "zuizhong:在一起\n",
      "0 aini 是真的\n",
      "1 henni 也是真的\n",
      "2 zuizhong 在一起\n"
     ]
    }
   ],
   "source": [
    "for name in aiqing:\n",
    "    print(f'{name}:{aiqing[name]}')\n",
    "\n",
    "for i, (name, score) in enumerate(aiqing.items()):           #字典的枚举 例举\n",
    "    print(i, name, score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2c8e4982",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['否', '是'])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numEntries=len(dataSet)\n",
    "labelCounts ={}  \n",
    "for featVec in  dataSet:       \n",
    "    currentLabel = featVec[-1]  #读取需要更新的类的具体明细\n",
    "    if currentLabel not in labelCounts.keys():   \n",
    "        labelCounts[currentLabel] =0\n",
    "    labelCounts[currentLabel] += 1\n",
    "labelCounts.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "78a8de2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "否\n",
      "是\n"
     ]
    }
   ],
   "source": [
    "for i in labelCounts.keys():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7f046f80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8791422543952"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a, b = 10,17\n",
    "  # [1, 2, 3, 4, 5, 6] \n",
    "id(a)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "590783f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84598992"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = a - b   # [1, 2, 3, 4, 5, 6]\n",
    "id(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "9169abc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84599728 -279\n"
     ]
    }
   ],
   "source": [
    "a -= b \n",
    "print(id(a),a)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "58eee5dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8791422543824"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a, b = 6,17\n",
    "  # [1, 2, 3, 4, 5, 6] \n",
    "id(a)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b9f0c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "dad6ff7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8791422547632 125\n"
     ]
    }
   ],
   "source": [
    "a += b \n",
    "print(id(a),a)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "d2c3998a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e7b0d698",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'return' outside function (<ipython-input-33-c0189d5d69f1>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-33-c0189d5d69f1>\"\u001b[1;36m, line \u001b[1;32m5\u001b[0m\n\u001b[1;33m    return shannonEnt\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m 'return' outside function\n"
     ]
    }
   ],
   "source": [
    "shannonEnt = 0.0\n",
    "for label in labelCounts.keys():\n",
    "    prob = float(labelCounts[label])/numEntries\n",
    "    shannonEnt -= prob*math.log(prob,2)\n",
    "return shannonEnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ae460f91",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-31-b85d5c100eab>, line 140)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-31-b85d5c100eab>\"\u001b[1;36m, line \u001b[1;32m140\u001b[0m\n\u001b[1;33m    else:                                              # 如果该特征时连续型数据\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from math import log\n",
    "import operator\n",
    "import numpy as np\n",
    "\n",
    "def createDataSet():\n",
    "    \"\"\"构建数据集\"\"\"\n",
    "    dataSet = [['是', '单身', 125, '否'],\n",
    "               ['否', '已婚', 100, '否'],\n",
    "               ['否', '单身', 70, '否'],\n",
    "               ['是', '已婚', 120, '否'],\n",
    "               ['否', '离异', 95, '是'],\n",
    "               ['否', '已婚', 60, '否'],\n",
    "               ['是', '离异', 220, '否'],\n",
    "               ['否', '单身', 85, '是'],\n",
    "               ['否', '已婚', 75, '否'],\n",
    "               ['否', '单身', 90, '是']]\n",
    "    labels = ['是否有房', '婚姻状况', '年收入(k)']  # 三个特征\n",
    "    return dataSet, labels\n",
    "\n",
    "#构建数据集合字段标签\n",
    "\n",
    "def calcShannonEnt(dataSet):\n",
    "    \"\"\"\n",
    "    计算给定数据集的香农熵\n",
    "    :param dataSet:给定的数据集\n",
    "    :return:返回香农熵\n",
    "    \"\"\"\n",
    "    numEntries = len(dataSet)     #数据集的记录条数 \n",
    "    labelCounts ={}               #生成一个空的字典\n",
    "    for featVec in  dataSet:       \n",
    "        currentLabel = featVec[-1]  #读取需要更新的类的具体明细\n",
    "        if currentLabel not in labelCounts.keys():   \n",
    "            labelCounts[currentLabel] =0\n",
    "        labelCounts[currentLabel] += 1\n",
    "    shannonEnt = 0.0\n",
    "    for label in labelCounts.keys():\n",
    "        prob = float(labelCounts[label])/numEntries\n",
    "        shannonEnt -= prob*log(prob,2)\n",
    "    return shannonEnt\n",
    "\n",
    "# 首先得到矩阵的长度，也就是结果的个数。\n",
    "# 建立一个空的字典。\n",
    "# #循环体\n",
    "# 将判定结果放到变量里，如果字典里没有，那就将其放到字典里。否则，就让他的值进行循环\n",
    "# 循环之后可以得到两种结果的个数\n",
    "\n",
    "# # 将标签的结果   写出来\n",
    "# -结果1的概率*log以2为底概率的对数-结果2的概率*log以2为底概率的对数\n",
    "# -e(i=1 到 n) p(x)*log(p(x))\n",
    "# -算出来最初始的信息熵\n",
    "# --用来计算信息熵\n",
    "\n",
    "def majorityCnt(classList):\n",
    "    \"\"\"获取出现次数最好的分类名称\"\"\"\n",
    "    classCount = {}\n",
    "    classList= np.mat(classList).flatten().A.tolist()[0]  # 数据为[['否'], ['是'], ['是']], 转换后为['否', '是', '是']\n",
    "    for vote in classList:\n",
    "        if vote not in classCount.keys():\n",
    "            classCount[vote] = 0\n",
    "        classCount[vote] += 1\n",
    "    sortedClassCount = sorted(classCount.items(),key=operator.itemgetter(1),reverse=True)\n",
    "    return sortedClassCount[0][0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def splitDataSet(dataSet,axis,value):\n",
    "    \"\"\"对离散型特征划分数据集\"\"\"\n",
    "    retDataSet = []  # 创建新的list对象，作为返回的数据\n",
    "    for featVec in dataSet:\n",
    "        if featVec[axis] == value:\n",
    "            reducedFeatVec = featVec[:axis]\n",
    "            reducedFeatVec.extend(featVec[axis+1:])  # 抽取\n",
    "            retDataSet.append(reducedFeatVec)\n",
    "    return retDataSet\n",
    "\n",
    "    \n",
    "    \n",
    "def splitContinuousDataSet(dataSet, axis, value, direction):\n",
    "    \"\"\"对连续型特征划分数据集\"\"\"\n",
    "    subDataSet = []\n",
    "    for featVec in dataSet:\n",
    "        if direction == 0:\n",
    "            if featVec[axis] > value:  # 按照大于(>)该值进行划分\n",
    "                reduceData = featVec[:axis]\n",
    "                reduceData.extend(featVec[axis + 1:])\n",
    "                subDataSet.append(reduceData)\n",
    "        if direction == 1:\n",
    "            if featVec[axis] <= value:  # 按照小于等于(<=)该值进行划分\n",
    "                reduceData = featVec[:axis]\n",
    "                reduceData.extend(featVec[axis + 1:])\n",
    "                subDataSet.append(reduceData)\n",
    "    return subDataSet\n",
    "\n",
    "def chooseBestFeatureToSplit(dataSet,labels):\n",
    "    \"\"\"选择最好的数据集划分方式\"\"\"\n",
    "    baseEntropy = calcShannonEnt(dataSet)\n",
    "    baseGainRatio = 0.0\n",
    "    bestFeature = -1\n",
    "    numFeatures = len(dataSet[0])-1\n",
    "    # 建立一个字典，用来存储每一个连续型特征所对应最佳切分点的具体值\n",
    "    bestSplitDic = {}\n",
    "    # print('dataSet[0]:' + str(dataSet[0]))\n",
    "    for i in range(numFeatures):\n",
    "        # 获取第i个特征的特征值\n",
    "        featVals = [example[i] for example in dataSet]\n",
    "        # 如果该特征时连续型数据\n",
    "        if type(featVals[0]).__name__ == 'float' or type(\n",
    "                featVals[0]).__name__ == 'int':\n",
    "            # 将该特征的所有值按从小到大顺序排序\n",
    "            sortedFeatVals = sorted(featVals)\n",
    "            # 取相邻两样本值的平均数做划分点，共有 len(featVals)-1 个\n",
    "            splitList = []\n",
    "            for j in range(len(featVals) - 1):\n",
    "                splitList.append(\n",
    "                    (sortedFeatVals[j] + sortedFeatVals[j + 1]) / 2.0)\n",
    "            # 遍历每一个切分点\n",
    "            for j in range(len(splitList)):\n",
    "                # 计算该划分方式的条件信息熵newEntropy\n",
    "                newEntropy = 0.0\n",
    "                value = splitList[j]\n",
    "                # 将数据集划分为两个子集\n",
    "                greaterSubDataSet = splitContinuousDataSet(dataSet, i, value, 0)\n",
    "                smallSubDataSet = splitContinuousDataSet(dataSet, i, value, 1)\n",
    "                prob0 = len(greaterSubDataSet) / float(len(dataSet))\n",
    "                newEntropy += prob0 * calcShannonEnt(greaterSubDataSet)\n",
    "                prob1 = len(smallSubDataSet) / float(len(dataSet))\n",
    "                newEntropy += prob1 * calcShannonEnt(smallSubDataSet)\n",
    "                # 计算该划分方式的分裂信息\n",
    "                splitInfo = 0.0\n",
    "                splitInfo -= prob0 * log(prob0, 2)\n",
    "                splitInfo -= prob1 * log(prob1, 2)\n",
    "                # 计算信息增益率 = 信息增益 / 该划分方式的分裂信息\n",
    "                gainRatio = float(baseEntropy - newEntropy) / splitInfo\n",
    "                if gainRatio > baseGainRatio:\n",
    "                    baseGainRatio = gainRatio\n",
    "                    bestSplit = j\n",
    "                    bestFeature = i\n",
    "            bestSplitDic[labels[i]] = splitList[bestSplit]  # 最佳切分点\n",
    "            else:                                              # 如果该特征时连续型数据\n",
    "            uniqueVals = set(featVals)\n",
    "            splitInfo = 0.0\n",
    "            # 计算每种划分方式的条件信息熵newEntropy\n",
    "            newEntropy = 0.0\n",
    "            for value in uniqueVals:\n",
    "                subDataSet = splitDataSet(dataSet, i, value)\n",
    "                prob = len(subDataSet)/float(len(dataSet))\n",
    "                splitInfo -= prob * log(prob, 2)  # 计算分裂信息\n",
    "                newEntropy += prob * calcShannonEnt(subDataSet)  # 计算条件信息熵\n",
    "            # 若该特征的特征值都相同，说明信息增益和分裂信息都为0，则跳过该特征\n",
    "            if splitInfo == 0.0:\n",
    "                continue\n",
    "            \n",
    "            \n",
    "            # 计算信息增益率 = 信息增益 / 该划分方式的分裂信息\n",
    "            \n",
    "            \n",
    "            gainRatio = float(baseEntropy - newEntropy) / splitInfo\n",
    "            if gainRatio > baseGainRatio:\n",
    "                bestFeature = i\n",
    "                baseGainRatio = gainRatio\n",
    "    # 如果最佳切分特征是连续型，则最佳切分点为具体的切分值\n",
    "    if type(dataSet[0][bestFeature]).__name__ == 'float' or type(\n",
    "            dataSet[0][bestFeature]).__name__ == 'int':\n",
    "        bestFeatValue = bestSplitDic[labels[bestFeature]]\n",
    "    # 如果最佳切分特征时离散型，则最佳切分点为 切分特征名称,【其实对于离散型特征这个值没有用】\n",
    "    if type(dataSet[0][bestFeature]).__name__ == 'str':\n",
    "        bestFeatValue = labels[bestFeature]\n",
    "    # print('bestFeature:' + str(labels[bestFeature]) + ', bestFeatValue:' + str(bestFeatValue))\n",
    "    #return bestFeature, bestFeatValue\n",
    "    print(\"最后的结果是%d%d\"bestFeature, bestFeatValue)      \n",
    "        \n",
    "def createTree(dataSet, labels):\n",
    "    \"\"\"创建C4.5树\"\"\"\n",
    "    classList = [example[-1] for example in dataSet]\n",
    "    # 如果类别完全相同，则停止继续划分\n",
    "    if classList.count(classList[0]) == len(classList):\n",
    "        return classList[0]\n",
    "    # 遍历完所有特征时返回出现次数最多的类别\n",
    "    if len(dataSet[0]) == 1:\n",
    "        return majorityCnt(classList)\n",
    "    bestFeature, bestFeatValue = chooseBestFeatureToSplit(dataSet, labels)\n",
    "    if bestFeature == -1:  # 如果无法选出最优分类特征，返回出现次数最多的类别\n",
    "        return majorityCnt(classList)\n",
    "    bestFeatLabel = labels[bestFeature]\n",
    "    myTree = {bestFeatLabel: {}}\n",
    "    subLabels = labels[:bestFeature]\n",
    "    subLabels.extend(labels[bestFeature+1:])\n",
    "    # 针对最佳切分特征是离散型\n",
    "    if type(dataSet[0][bestFeature]).__name__ == 'str':\n",
    "        featVals = [example[bestFeature] for example in dataSet]\n",
    "        uniqueVals = set(featVals)\n",
    "        for value in uniqueVals:\n",
    "            reduceDataSet = splitDataSet(dataSet, bestFeature, value)\n",
    "            # print('reduceDataSet:' + str(reduceDataSet))\n",
    "            myTree[bestFeatLabel][value] = createTree(reduceDataSet,subLabels)\n",
    "            # print(myTree[bestFeatLabel][value])\n",
    "    # 针对最佳切分特征是连续型\n",
    "    if type(dataSet[0][bestFeature]).__name__ == 'int' or type(\n",
    "            dataSet[0][bestFeature]).__name__ == 'float':\n",
    "        # 将数据集划分为两个子集，针对每个子集分别建树\n",
    "        value = bestFeatValue\n",
    "        greaterSubDataSet = splitContinuousDataSet(dataSet,bestFeature, value, 0)\n",
    "        smallSubDataSet = splitContinuousDataSet(dataSet,bestFeature,value,1)\n",
    "        # print('greaterDataset:' + str(greaterSubDataSet))\n",
    "        # print('smallerDataSet:' + str(smallSubDataSet))\n",
    "        # 针对连续型特征，在生成决策的模块，修改划分点的标签，如“> x.xxx”，\"<= x.xxx\"\n",
    "        myTree[bestFeatLabel]['>' + str(value)] = createTree(greaterSubDataSet,subLabels)\n",
    "        myTree[bestFeatLabel]['<=' + str(value)] = createTree(smallSubDataSet,subLabels)\n",
    "    return myTree\n",
    "if __name__ == '__main__':\n",
    "    dataSet, labels = createDataSet()\n",
    "    mytree = createTree(dataSet, labels)\n",
    "    print(\"最终构建的C4.5分类树为：\\n\",mytree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebef801f",
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct 生死与生命"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2599739",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf3cd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "//曾近相信我会相信你我会相信你 你不会把爱我说的这么清新脱俗"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dfc4a5d2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'classList' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-7dd5acf1d2f8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassList\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'classList' is not defined"
     ]
    }
   ],
   "source": [
    "np.mat(classList).flatten().A.tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80101b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "识别    处理    执行   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd030e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "我们会得到一个类似于人的大脑的系统，只要输入图片，我就可以得到我所想知道的信息。   \n",
    "那也就是说，结果相较于自变量，并不是一个线性映射。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1052a28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"最终构建的C4.5分类树为：\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d4be90",
   "metadata": {},
   "source": [
    "##    就决策树的这个算法而言,在非常多的行业都会有应用,是不是使用决策树进行挖掘分析个人认为还是要看具体的应用分析目标,广义点说任何一个行业都可能出现适合决策树的应用分析目标,比如:在用决策树进行用户分级评估的时候,凡是积累了一定量的客户资源和数据, 涉及对自己行业客户进行深入分析的企业和分析者都可能具备使用决策树的条件。\n",
    "##    一般来说决策树的应用用往往都是和某一应用分析目标和场景相关的,比如:金融行业可以用决策树做贷款风险评估,保险行业可以用决策树做险种推广预测,医疗行业可以用决策树生成辅助诊断处置模型等等,当一个决策树的应用分析目标和场景确定,那该应用分析目标和场景所处的行业也就自然成为了决策树的应用领域。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a186b9",
   "metadata": {},
   "source": [
    "# 决策树算法的数学基础"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb00b0a3",
   "metadata": {},
   "source": [
    "## 构造：就是哪个属性是根节点，哪个属性是叶子节点"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225b9b41",
   "metadata": {},
   "source": [
    "## 剪枝\n",
    "## 剪枝就是给决策树瘦身，这一步想实现的目标就是，不需要太多的判断，同样可以得到不错的结果。之所以这么做，是为了防止“过拟合”（Overfitting）现象的发生。\n",
    "## 欠拟合，和过拟合就好比是下面这张图中的第一个和第三个情况一样，训练的结果“太好“，反而在实际应用过程中会导致分类错误。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935e6046",
   "metadata": {},
   "source": [
    "#### 预剪枝是在决策树构造时就进行剪枝。方法是在构造的过程中对节点进行评估，如果对某个节点进行划分，在验证集中不能带来准确性的提升，那么对这个节点进行划分就没有意义，这时就会把当前节点作为叶节点，不对其进行划分。\n",
    "#### 后剪枝就是在生成决策树之后再进行剪枝，通常会从决策树的叶节点开始，逐层向上对每个节点进行评估。如果剪掉这个节点子树（对该节点进行劈叉），分类准确性没有变化或是能带来准确性的提升，那么就可以把该节点子树进行剪枝。方法是：用这个节点子树的叶子节点来替代该节点，类标记为这个节点子树中最频繁的那个类。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c0c392",
   "metadata": {},
   "source": [
    "# 当不确定性越大时，它所包含的信息量也就越大，信息熵也就越高。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fff945b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7faea871",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1, 2, 3]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ac53190b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'matrix' object has no attribute 'm'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-c77873c08a3d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m##.A.tolist()[0]  # 数据为\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'matrix' object has no attribute 'm'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcb4b755",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'treePlotter'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-5a2bd29c267f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmath\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlog\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0moperator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtreePlotter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcreateDataSet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'treePlotter'"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "#encoding:utf-8\n",
    "#decision_tree.py\n",
    "from math import log\n",
    "import operator\n",
    "import treePlotter\n",
    "\n",
    "def createDataSet():\n",
    "    dataSet = [[1, 1, 'yes'],\n",
    "               [1, 1, 'yes'],\n",
    "               [1, 0, 'no'],\n",
    "               [0, 1, 'no'],\n",
    "               [0, 1, 'no']]\n",
    "    labels = ['no surfacing','flippers']\n",
    "    #change to discrete values\n",
    "    return dataSet, labels\n",
    "\n",
    "##### 计算信息熵 ######\n",
    "def calcShannonEnt(dataSet):\n",
    "    numEntries = len(dataSet)  # 样本数\n",
    "    labelCounts = {}   # 创建一个数据字典：key是最后一列的数值（即标签，也就是目标分类的类别），value是属于该类别的样本个数\n",
    "    for featVec in dataSet: # 遍历整个数据集，每次取一行\n",
    "        currentLabel = featVec[-1]  #取该行最后一列的值\n",
    "        if currentLabel not in labelCounts.keys(): labelCounts[currentLabel] = 0\n",
    "        labelCounts[currentLabel] += 1\n",
    "    shannonEnt = 0.0  # 初始化信息熵\n",
    "    for key in labelCounts:\n",
    "        prob = float(labelCounts[key])/numEntries\n",
    "        shannonEnt -= prob * log(prob,2) #log base 2  计算信息熵\n",
    "    return shannonEnt\n",
    "\n",
    "##### 按给定的特征划分数据 #########\n",
    "def splitDataSet(dataSet, axis, value): #axis是dataSet数据集下要进行特征划分的列号例如outlook是0列，value是该列下某个特征值，0列中的sunny\n",
    "    retDataSet = []\n",
    "    for featVec in dataSet: #遍历数据集，并抽取按axis的当前value特征进划分的数据集(不包括axis列的值)\n",
    "        if featVec[axis] == value: #\n",
    "            reducedFeatVec = featVec[:axis]     #chop out axis used for splitting\n",
    "            reducedFeatVec.extend(featVec[axis+1:])\n",
    "            retDataSet.append(reducedFeatVec)\n",
    "            # print axis,value,reducedFeatVec\n",
    "    # print retDataSet\n",
    "    return retDataSet\n",
    "\n",
    "##### 选取当前数据集下，用于划分数据集的最优特征\n",
    "def chooseBestFeatureToSplit(dataSet):\n",
    "    numFeatures = len(dataSet[0]) - 1      #获取当前数据集的特征个数，最后一列是分类标签\n",
    "    baseEntropy = calcShannonEnt(dataSet)  #计算当前数据集的信息熵\n",
    "    bestInfoGain = 0.0; bestFeature = -1   #初始化最优信息增益和最优的特征\n",
    "    for i in range(numFeatures):        #遍历每个特征iterate over all the features\n",
    "        featList = [example[i] for example in dataSet]#获取数据集中当前特征下的所有值\n",
    "        uniqueVals = set(featList)       # 获取当前特征值，例如outlook下有sunny、overcast、rainy\n",
    "        newEntropy = 0.0\n",
    "        for value in uniqueVals: #计算每种划分方式的信息熵\n",
    "            subDataSet = splitDataSet(dataSet, i, value)\n",
    "            prob = len(subDataSet)/float(len(dataSet))\n",
    "            newEntropy += prob * calcShannonEnt(subDataSet)\n",
    "        infoGain = baseEntropy - newEntropy     #计算信息增益\n",
    "        if (infoGain > bestInfoGain):       #比较每个特征的信息增益，只要最好的信息增益\n",
    "            bestInfoGain = infoGain         #if better than current best, set to best\n",
    "            bestFeature = i\n",
    "    return bestFeature                      #returns an integer\n",
    "#####该函数使用分类名称的列表，然后创建键值为classList中唯一值的数据字典。字典\n",
    "#####对象的存储了classList中每个类标签出现的频率。最后利用operator操作键值排序字典，\n",
    "#####并返回出现次数最多的分类名称\n",
    "def majorityCnt(classList):\n",
    "    classCount={}\n",
    "    for vote in classList:\n",
    "        if vote not in classCount.keys(): classCount[vote] = 0\n",
    "        classCount[vote] += 1\n",
    "    sortedClassCount = sorted(classCount.iteritems(), key=operator.itemgetter(1), reverse=True)\n",
    "    return sortedClassCount[0][0]\n",
    "\n",
    "##### 生成决策树主方法\n",
    "def createTree(dataSet,labels):\n",
    "    classList = [example[-1] for example in dataSet] # 返回当前数据集下标签列所有值\n",
    "    if classList.count(classList[0]) == len(classList):\n",
    "        return classList[0]#当类别完全相同时则停止继续划分，直接返回该类的标签\n",
    "    if len(dataSet[0]) == 1: ##遍历完所有的特征时，仍然不能将数据集划分成仅包含唯一类别的分组 dataSet\n",
    "        return majorityCnt(classList) #由于无法简单的返回唯一的类标签，这里就返回出现次数最多的类别作为返回值\n",
    "    bestFeat = chooseBestFeatureToSplit(dataSet) # 获取最好的分类特征索引\n",
    "    bestFeatLabel = labels[bestFeat] #获取该特征的名字\n",
    "\n",
    "    # 这里直接使用字典变量来存储树信息，这对于绘制树形图很重要。\n",
    "    myTree = {bestFeatLabel:{}} #当前数据集选取最好的特征存储在bestFeat中\n",
    "    del(labels[bestFeat]) #删除已经在选取的特征\n",
    "    featValues = [example[bestFeat] for example in dataSet]\n",
    "    uniqueVals = set(featValues)\n",
    "    for value in uniqueVals:\n",
    "        subLabels = labels[:]       #copy all of labels, so trees don't mess up existing labels\n",
    "        myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet, bestFeat, value),subLabels)\n",
    "    return myTree\n",
    "\n",
    "def classify(inputTree,featLabels,testVec):\n",
    "    firstStr = inputTree.keys()[0]\n",
    "    secondDict = inputTree[firstStr]\n",
    "    featIndex = featLabels.index(firstStr)\n",
    "    key = testVec[featIndex]\n",
    "    valueOfFeat = secondDict[key]\n",
    "    if isinstance(valueOfFeat, dict):\n",
    "        classLabel = classify(valueOfFeat, featLabels, testVec)\n",
    "    else: classLabel = valueOfFeat\n",
    "    return classLabel\n",
    "\n",
    "def storeTree(inputTree,filename):\n",
    "    import pickle\n",
    "    fw = open(filename,'w')\n",
    "    pickle.dump(inputTree,fw)\n",
    "    fw.close()\n",
    "\n",
    "def grabTree(filename):\n",
    "    import pickle\n",
    "    fr = open(filename)\n",
    "    return pickle.load(fr)\n",
    "\n",
    "#outlook','temperature','huminidy','windy\n",
    "fr = \\\n",
    "['sunny hot high FALSE no',\n",
    "'sunny hot high TRUE no',\n",
    "'overcast hot high FALSE yes',\n",
    "'rainy mild high FALSE yes',\n",
    "'rainy cool normal FALSE yes',\n",
    "'rainy cool normal TRUE no',\n",
    "'overcast cool normal TRUE yes',\n",
    "'sunny mild high FALSE no',\n",
    "'sunny cool normal FALSE yes',\n",
    "'rainy mild normal FALSE yes',\n",
    "'sunny mild normal TRUE yes',\n",
    "'overcast mild high TRUE yes',\n",
    "'overcast hot normal FALSE yes',\n",
    "'rainy mild high TRUE no']\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    lenses =[inst.strip().split(' ') for inst in fr]\n",
    "    lensesLabels = ['outlook','temperature','huminidy','windy']\n",
    "    lensesTree =createTree(lenses,lensesLabels)\n",
    "    treePlotter.createPlot(lensesTree)\n",
    "\n",
    "\n",
    "\n",
    "#!/usr/bin/python\n",
    "#encoding:utf-8\n",
    "#treePlotter.py\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "decisionNode = dict(boxstyle=\"sawtooth\", fc=\"0.8\") #定义文本框与箭头的格式\n",
    "leafNode = dict(boxstyle=\"round4\", fc=\"0.8\")\n",
    "arrow_args = dict(arrowstyle=\"<-\")\n",
    "\n",
    "def getNumLeafs(myTree): #获取树叶节点的数目\n",
    "    numLeafs = 0\n",
    "    firstStr = list(myTree.keys())[0]\n",
    "    secondDict = myTree[firstStr]\n",
    "    for key in secondDict.keys():\n",
    "        if type(secondDict[key]).__name__=='dict':#测试节点的数据类型是不是字典，如果是则就需要递归的调用getNumLeafs()函数\n",
    "            numLeafs += getNumLeafs(secondDict[key])\n",
    "        else:   numLeafs +=1\n",
    "    return numLeafs\n",
    "\n",
    "def getTreeDepth(myTree): #获取树的深度\n",
    "    maxDepth = 0\n",
    "    firstStr = list(myTree.keys())[0]\n",
    "    secondDict = myTree[firstStr]\n",
    "    for key in secondDict.keys():\n",
    "        if type(secondDict[key]).__name__=='dict':#test to see if the nodes are dictonaires, if not they are leaf nodes\n",
    "            thisDepth = 1 + getTreeDepth(secondDict[key])\n",
    "        else:   thisDepth = 1\n",
    "        if thisDepth > maxDepth: maxDepth = thisDepth\n",
    "    return maxDepth\n",
    "\n",
    "# 绘制带箭头的注释\n",
    "def plotNode(nodeTxt, centerPt, parentPt, nodeType):\n",
    "    createPlot.ax1.annotate(nodeTxt, xy=parentPt,  xycoords='axes fraction',\n",
    "             xytext=centerPt, textcoords='axes fraction',\n",
    "             va=\"center\", ha=\"center\", bbox=nodeType, arrowprops=arrow_args )\n",
    "\n",
    "#计算父节点和子节点的中间位置，在父节点间填充文本的信息\n",
    "def plotMidText(cntrPt, parentPt, txtString):\n",
    "    xMid = (parentPt[0]-cntrPt[0])/2.0 + cntrPt[0]\n",
    "    yMid = (parentPt[1]-cntrPt[1])/2.0 + cntrPt[1]\n",
    "    createPlot.ax1.text(xMid, yMid, txtString, va=\"center\", ha=\"center\", rotation=30)\n",
    "\n",
    "# 画决策树的准备方法\n",
    "def plotTree(myTree, parentPt, nodeTxt):#if the first key tells you what feat was split on\n",
    "    numLeafs = getNumLeafs(myTree)  #计算树的宽度\n",
    "    depth = getTreeDepth(myTree)   #计算树的深度\n",
    "    firstStr = list(myTree.keys())[0] #the text label for this node should be this\n",
    "    cntrPt = (plotTree.xOff + (1.0 + float(numLeafs))/2.0/plotTree.totalW, plotTree.yOff)\n",
    "    plotMidText(cntrPt, parentPt, nodeTxt)\n",
    "    plotNode(firstStr, cntrPt, parentPt, decisionNode)\n",
    "    secondDict = myTree[firstStr]\n",
    "    plotTree.yOff = plotTree.yOff - 1.0/plotTree.totalD\n",
    "    for key in secondDict.keys():\n",
    "        if type(secondDict[key]).__name__=='dict':#test to see if the nodes are dictonaires, if not they are leaf nodes\n",
    "            plotTree(secondDict[key],cntrPt,str(key))        #recursion\n",
    "        else:   #it's a leaf node print the leaf node\n",
    "            plotTree.xOff = plotTree.xOff + 1.0/plotTree.totalW\n",
    "            plotNode(secondDict[key], (plotTree.xOff, plotTree.yOff), cntrPt, leafNode)\n",
    "            plotMidText((plotTree.xOff, plotTree.yOff), cntrPt, str(key))\n",
    "    plotTree.yOff = plotTree.yOff + 1.0/plotTree.totalD\n",
    "#if you do get a dictonary you know it's a tree, and the first element will be another dict\n",
    "\n",
    "\n",
    "# 画决策树主方法\n",
    "def createPlot(inTree):\n",
    "    fig = plt.figure(1, facecolor='white')\n",
    "    fig.clf()\n",
    "    axprops = dict(xticks=[], yticks=[])\n",
    "    createPlot.ax1 = plt.subplot(111, frameon=False, **axprops)    #no ticks\n",
    "    plotTree.totalW = float(getNumLeafs(inTree))\n",
    "    plotTree.totalD = float(getTreeDepth(inTree))\n",
    "    plotTree.xOff = -0.5/plotTree.totalW; plotTree.yOff = 1.0;\n",
    "    plotTree(inTree, (0.5,1.0), '')\n",
    "    plt.show()\n",
    "\n",
    "def retrieveTree(i):\n",
    "    listOfTrees =[{'no surfacing': {0: 'no', 1: {'flippers': {0: 'no', 1: 'yes'}}}},\n",
    "                  {'no surfacing': {0: 'no', 1: {'flippers': {0: {'head': {0: 'no', 1: 'yes'}}, 1: 'no'}}}}\n",
    "                  ]\n",
    "    return listOfTrees[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a31c1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33d383a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
